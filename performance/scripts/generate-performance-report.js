#!/usr/bin/env node

/**
 * Performance Report Generator
 * Generates performance reports from test results
 */

const fs = require('fs')
const path = require('path')

function parsePerformanceResults() {
  // Read from performance-metrics.json (generated by tests)
  const metricsPath = path.resolve(
    'performance/results/performance-metrics.json'
  )

  if (!fs.existsSync(metricsPath)) {
    const errorMsg = `Performance metrics file not found: ${metricsPath}. Run performance tests first.`
    console.error(`‚ùå ${errorMsg}`)
    throw new Error(errorMsg)
  }

  try {
    console.log('‚úÖ Reading metrics from performance-metrics.json')
    const metrics = JSON.parse(fs.readFileSync(metricsPath, 'utf8'))

    // Validate metrics structure
    if (typeof metrics !== 'object' || metrics === null) {
      throw new Error('Metrics file does not contain a valid object')
    }

    const metricCount = Object.keys(metrics).length
    console.log(`üìä Loaded ${metricCount} performance metrics`)

    // Log any unusual values
    Object.entries(metrics).forEach(([key, value]) => {
      if (typeof value !== 'number' || isNaN(value) || value < 0) {
        console.warn(`‚ö†Ô∏è Unusual metric value for ${key}: ${value}`)
      }
    })

    return metrics
  } catch (error) {
    console.error(`‚ùå Failed to parse performance metrics: ${error.message}`)
    throw error
  }
}

function generateMarkdownReport(
  metrics,
  title = 'Performance Benchmark Results'
) {
  let report = `# üöÄ ${title}\n\n`

  report += '### üìä Performance Metrics\n\n'
  report += '| Metric | Value | Status |\n'
  report += '|--------|-------|--------|\n'

  Object.entries(metrics).forEach(([key, value]) => {
    const unit = key.includes('Calls') ? 'calls' : 'ms'
    const status = getStatus(key, value)
    report += `| ${key} | ${value}${unit} | ${status} |\n`
  })

  report += '\n### üìà Performance Targets\n\n'
  report += '| Metric | Target | Notes |\n'
  report += '|--------|--------|-------|\n'
  report += '| Calendar Render (1 month) | < 100ms | Initial load time |\n'
  report += '| Calendar Render (3 months) | < 200ms | Multi-month view |\n'
  report +=
    '| Re-render (unchanged props) | < 25ms | Memoization effectiveness |\n'
  report += '| Month Navigation | < 75ms | User interaction |\n'
  report +=
    '| Array.from Calls (Re-render) | 0 calls | Static array optimization |\n'
  report += '| DtPicker Render | < 150ms | Component initialization |\n'
  report += '| DtPicker Modal Open | < 100ms | Modal interaction |\n'

  report += '\n_Generated on ' + new Date().toISOString() + '_\n'

  return report
}

function getStatus(metric, value) {
  if (metric.includes('Calls')) {
    return value === 0 ? '‚úÖ PASS' : '‚ö†Ô∏è CHECK'
  }

  const targets = {
    'DtCalendar (1 month)': 100,
    'DtCalendar (3 months)': 200,
    'Re-render': 25,
    'Month Navigation': 75,
    'DtPicker Render': 150,
    'DtPicker Modal Open': 100,
    'Memoized Grid Navigation': 30
  }

  // Find matching target
  const targetKey = Object.keys(targets).find((key) => metric.includes(key))
  if (!targetKey) return '‚ùì UNKNOWN'

  const target = targets[targetKey]
  if (value <= target) return '‚úÖ PASS'
  if (value <= target * 1.5) return '‚ö†Ô∏è SLOW'
  return '‚ùå FAIL'
}

function compareMetrics(baseline, current) {
  const comparison = {}

  Object.keys(current).forEach((key) => {
    const currentVal = current[key]
    const baselineVal = baseline[key]

    if (baselineVal === undefined) {
      comparison[key] = { current: currentVal, change: 'NEW', status: '‚ûï NEW' }
      return
    }

    const isCalls = key.includes('Calls')
    const change = currentVal - baselineVal

    let status, changeDesc
    if (isCalls) {
      // For call counts, lower is better
      changeDesc = change > 0 ? `+${change}` : change.toString()
      status =
        change < 0 ? '‚úÖ IMPROVED' : change > 0 ? '‚ùå DEGRADED' : '‚û°Ô∏è SAME'
    } else {
      // For time metrics, lower is better
      const percentChange = baselineVal > 0 ? (change / baselineVal) * 100 : 0
      changeDesc = change >= 0 ? `+${change.toFixed(2)}` : change.toFixed(2)
      status =
        change < 0
          ? `‚úÖ IMPROVED (${Math.abs(percentChange).toFixed(1)}% faster)`
          : change > 0
            ? `‚ùå DEGRADED (${percentChange.toFixed(1)}% slower)`
            : '‚û°Ô∏è SAME'
    }

    let diffIndicator
    if (status.includes('IMPROVED')) {
      diffIndicator = 'üü¢'
    } else if (status.includes('DEGRADED')) {
      diffIndicator = 'üî¥'
    } else if (status.includes('SAME')) {
      diffIndicator = '‚ö™'
    } else {
      diffIndicator = '‚ùì'
    }

    comparison[key] = {
      baseline: baselineVal,
      current: currentVal,
      change: changeDesc,
      diffIndicator,
      status
    }
  })

  return comparison
}

function generateComparisonReport(baselineMetrics, currentMetrics) {
  const comparison = compareMetrics(baselineMetrics, currentMetrics)

  let report = '# ‚ö° Performance Comparison\n\n'
  report += '| Metric | Baseline | Current | Diff | Change | Status |\n'
  report += '|--------|----------|---------|------|--------|--------|\n'

  Object.entries(comparison).forEach(([key, data]) => {
    const unit = key.includes('Calls') ? 'calls' : 'ms'
    const baselineStr =
      data.baseline !== undefined ? `${data.baseline}${unit}` : 'N/A'
    const currentStr = `${data.current}${unit}`

    report += `| ${key} | ${baselineStr} | ${currentStr} | ${data.diffIndicator} ${data.change}${unit} | ${data.change}${unit} | ${data.status} |\n`
  })

  // Summary
  const improved = Object.values(comparison).filter((item) =>
    item.status.includes('IMPROVED')
  ).length
  const degraded = Object.values(comparison).filter((item) =>
    item.status.includes('DEGRADED')
  ).length
  const same = Object.values(comparison).filter((item) =>
    item.status.includes('SAME')
  ).length
  const newMetrics = Object.values(comparison).filter((item) =>
    item.status.includes('NEW')
  ).length

  report += '\n## üìä Summary\n\n'
  report += `- **${improved}** metrics improved\n`
  report += `- **${degraded}** metrics degraded\n`
  report += `- **${same}** metrics unchanged\n`
  report += `- **${newMetrics}** new metrics\n`

  report += '\n_Performance comparison generated automatically_\n'

  return report
}

// CLI interface
const args = process.argv.slice(2)
const command = args[0]

switch (command) {
  case 'report': {
    const resultsPath = args[1] || 'performance-results.json'
    const outputPath = args[2] || 'performance-report.md'

    try {
      const metrics = parsePerformanceResults(resultsPath)
      const report = generateMarkdownReport(metrics)

      fs.writeFileSync(outputPath, report)
      console.log(`Performance report generated: ${outputPath}`)

      // Also save metrics as JSON for comparisons
      fs.writeFileSync(
        'performance/results/performance-metrics.json',
        JSON.stringify(metrics, null, 2)
      )
      console.log(
        'Metrics saved as: performance/results/performance-metrics.json'
      )
    } catch (error) {
      console.error('Error generating report:', error.message)
      process.exit(1)
    }
    break
  }

  case 'compare': {
    const baselinePath = args[1] || 'baseline-metrics.json'
    const currentPath = args[2] || 'performance-metrics.json'
    const compareOutputPath = args[3] || 'performance-comparison.md'

    try {
      const baseline = JSON.parse(fs.readFileSync(baselinePath, 'utf8'))
      const current = JSON.parse(fs.readFileSync(currentPath, 'utf8'))

      const report = generateComparisonReport(baseline, current)
      fs.writeFileSync(compareOutputPath, report)

      console.log(`Performance comparison generated: ${compareOutputPath}`)
    } catch (error) {
      console.error('Error generating comparison:', error.message)
      process.exit(1)
    }
    break
  }

  default: {
    console.log('Usage:')
    console.log(
      '  node performance/scripts/generate-performance-report.js report [results.json] [output.md]'
    )
    console.log(
      '  node performance/scripts/generate-performance-report.js compare [baseline.json] [current.json] [output.md]'
    )
    console.log('')
    console.log('Examples:')
    console.log('  node performance/scripts/generate-performance-report.js report')
    console.log(
      '  node performance/scripts/generate-performance-report.js compare baseline.json performance-metrics.json'
    )
    break
  }
}
