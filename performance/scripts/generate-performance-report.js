#!/usr/bin/env node

/**
 * Performance Report Generator
 * Generates performance reports from test results
 */

const fs = require('fs')
const path = require('path')

function parsePerformanceResults(metricsPath) {
  // Read from performance-metrics.json (generated by tests)
  const resolvedPath = metricsPath
    ? path.resolve(metricsPath)
    : path.resolve('performance/results/performance-metrics.json')

  if (!fs.existsSync(resolvedPath)) {
    const errorMsg = `Performance metrics file not found: ${resolvedPath}. Run performance tests first.`
    console.error(`‚ùå ${errorMsg}`)
    throw new Error(errorMsg)
  }

  try {
    console.log(`‚úÖ Reading metrics from ${resolvedPath}`)
    const metrics = JSON.parse(fs.readFileSync(resolvedPath, 'utf8'))

    // Validate metrics structure
    if (typeof metrics !== 'object' || metrics === null) {
      throw new Error('Metrics file does not contain a valid object')
    }

    const metricCount = Object.keys(metrics).length
    console.log(`üìä Loaded ${metricCount} performance metrics`)

    // Log any unusual values
    Object.entries(metrics).forEach(([key, value]) => {
      if (typeof value !== 'number' || isNaN(value) || value < 0) {
        console.warn(`‚ö†Ô∏è Unusual metric value for ${key}: ${value}`)
      }
    })

    return metrics
  } catch (error) {
    console.error(`‚ùå Failed to parse performance metrics: ${error.message}`)
    throw error
  }
}

function getTarget(metric) {
  const targets = {
    'DtCalendar (1 month)': '< 100ms',
    'DtCalendar (3 months)': '< 200ms',
    'Re-render (unchanged props)': '< 25ms',
    'Month Navigation': '< 75ms',
    'Array.from Calls (Re-render)': '0 calls',
    'Array.from Calls': '0 calls',
    'DtPicker Render': '< 150ms',
    'DtPicker Modal Open': '< 100ms',
    'Memoized Grid Navigation': '< 30ms'
  }

  // Find matching target (check if metric name contains any target key)
  // Sort by length (longest first) to match more specific keys first
  const sortedKeys = Object.keys(targets).sort((a, b) => b.length - a.length)

  for (const targetKey of sortedKeys) {
    if (metric.includes(targetKey)) {
      return targets[targetKey]
    }
  }

  return 'N/A'
}

function formatValue(value, isCalls) {
  if (isCalls) {
    return Math.round(value).toString()
  }
  return value.toFixed(2)
}

function formatBytes(bytes) {
  if (bytes === 0) return '0 B'
  const k = 1024
  const sizes = ['B', 'kB', 'MB', 'GB']
  const i = Math.floor(Math.log(bytes) / Math.log(k))
  return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i]
}

function getBundleStatus(asset, size) {
  const limits = {
    'index.mjs': 25 * 1024, // 25 kB gzip
    'index.cjs': 20 * 1024, // 20 kB gzip
    'style.css': 5 * 1024 // 5 kB gzip
  }

  const limit = limits[asset]
  if (!limit) return '‚ùì UNKNOWN'

  return size <= limit ? '‚úÖ PASS' : '‚ùå FAIL'
}

function getComparisonValue(current, baseline, isCalls) {
  if (baseline === undefined || baseline === null) {
    return 'null'
  }

  const diff = current - baseline
  if (isCalls) {
    return diff > 0 ? `+${Math.round(diff)}` : Math.round(diff).toString()
  }
  return diff >= 0 ? `+${diff.toFixed(2)}` : diff.toFixed(2)
}

function generateMarkdownReport(
  metrics,
  title = 'Performance Benchmark Results',
  baselineMetrics = null,
  bundleMetrics = null
) {
  let report = `# üöÄ ${title}\n\n`

  // Bundle Size Section (if available)
  if (bundleMetrics) {
    report += '### üì¶ Bundle Size\n\n'
    report += '| Asset | Size | Gzipped | Status |\n'
    report += '|-------|------|---------|--------|\n'

    Object.entries(bundleMetrics).forEach(([key, value]) => {
      const isGzip = key.includes('(gzip)')
      const baseName = key.replace(' (gzip)', '').replace(' (raw)', '')
      const size = isGzip ? value : bundleMetrics[`${baseName} (gzip)`] || value
      const status = getBundleStatus(baseName, size)
      const sizeStr = formatBytes(value)

      if (isGzip) {
        // Find corresponding raw size
        const rawKey = key.replace(' (gzip)', ' (raw)')
        const rawSize = bundleMetrics[rawKey]
        report += `| ${baseName} | ${formatBytes(rawSize)} | ${sizeStr} | ${status} |\n`
      }
    })
    report += '\n'
  }

  report += '### ‚ö° Performance Metrics\n\n'
  report += '| Metric | Value | Target | Comparison | Status |\n'
  report += '|--------|-------|--------|------------|--------|\n'

  Object.entries(metrics).forEach(([key, value]) => {
    const isCalls = key.includes('Calls')
    const unit = isCalls ? 'calls' : 'ms'
    const formattedValue = formatValue(value, isCalls)
    const target = getTarget(key)
    const status = getStatus(key, value)

    // Get comparison value (difference from baseline)
    const baselineValue = baselineMetrics ? baselineMetrics[key] : null
    const comparison = getComparisonValue(value, baselineValue, isCalls)
    const comparisonStr =
      comparison === 'null' ? 'null' : `${comparison}${unit}`

    report += `| ${key} | ${formattedValue}${unit} | ${target} | ${comparisonStr} | ${status} |\n`
  })

  report += '\n_Generated on ' + new Date().toISOString() + '_\n'

  return report
}

function getStatus(metric, value) {
  if (metric.includes('Calls')) {
    return value === 0 ? '‚úÖ PASS' : '‚ö†Ô∏è CHECK'
  }

  const targets = {
    'DtCalendar (1 month)': 100,
    'DtCalendar (3 months)': 200,
    'Re-render (unchanged props)': 25,
    'Month Navigation': 75,
    'DtPicker Render': 150,
    'DtPicker Modal Open': 100,
    'Memoized Grid Navigation': 30
  }

  // Find matching target (sort by length to match more specific keys first)
  const sortedKeys = Object.keys(targets).sort((a, b) => b.length - a.length)
  const targetKey = sortedKeys.find((key) => metric.includes(key))
  if (!targetKey) return '‚ùì UNKNOWN'

  const target = targets[targetKey]
  if (value <= target) return '‚úÖ PASS'
  if (value <= target * 1.5) return '‚ö†Ô∏è SLOW'
  return '‚ùå FAIL'
}

function compareMetrics(baseline, current) {
  const comparison = {}

  Object.keys(current).forEach((key) => {
    const currentVal = current[key]
    const baselineVal =
      baseline && baseline[key] !== undefined ? baseline[key] : null

    if (baselineVal === null || baselineVal === undefined) {
      comparison[key] = {
        current: currentVal,
        change: 'null',
        status: '‚ûï NEW'
      }
      return
    }

    const isCalls = key.includes('Calls')
    const change = currentVal - baselineVal

    let status, changeDesc
    if (isCalls) {
      // For call counts, lower is better
      changeDesc =
        change > 0 ? `+${Math.round(change)}` : Math.round(change).toString()
      status =
        change < 0 ? '‚úÖ IMPROVED' : change > 0 ? '‚ùå DEGRADED' : '‚û°Ô∏è SAME'
    } else {
      // For time metrics, lower is better
      const percentChange = baselineVal > 0 ? (change / baselineVal) * 100 : 0
      changeDesc = change >= 0 ? `+${change.toFixed(2)}` : change.toFixed(2)
      status =
        change < 0
          ? `‚úÖ IMPROVED (${Math.abs(percentChange).toFixed(1)}% faster)`
          : change > 0
            ? `‚ùå DEGRADED (${percentChange.toFixed(1)}% slower)`
            : '‚û°Ô∏è SAME'
    }

    let diffIndicator
    if (status.includes('IMPROVED')) {
      diffIndicator = 'üü¢'
    } else if (status.includes('DEGRADED')) {
      diffIndicator = 'üî¥'
    } else if (status.includes('SAME')) {
      diffIndicator = '‚ö™'
    } else {
      diffIndicator = '‚ùì'
    }

    comparison[key] = {
      baseline: baselineVal,
      current: currentVal,
      change: changeDesc,
      diffIndicator,
      status
    }
  })

  return comparison
}

function generateComparisonReport(baselineMetrics, currentMetrics) {
  const comparison = compareMetrics(baselineMetrics, currentMetrics)

  let report = '# ‚ö° Performance Comparison\n\n'
  report += '| Metric | Baseline | Current | Diff | Change | Status |\n'
  report += '|--------|----------|---------|------|--------|--------|\n'

  Object.entries(comparison).forEach(([key, data]) => {
    const unit = key.includes('Calls') ? 'calls' : 'ms'
    const isCalls = key.includes('Calls')
    const baselineStr =
      data.baseline !== null && data.baseline !== undefined
        ? `${formatValue(data.baseline, isCalls)}${unit}`
        : 'null'
    const currentStr = `${formatValue(data.current, isCalls)}${unit}`

    report += `| ${key} | ${baselineStr} | ${currentStr} | ${data.diffIndicator} ${data.change === 'null' ? 'null' : data.change + unit} | ${data.change === 'null' ? 'null' : data.change + unit} | ${data.status} |\n`
  })

  // Summary
  const improved = Object.values(comparison).filter((item) =>
    item.status.includes('IMPROVED')
  ).length
  const degraded = Object.values(comparison).filter((item) =>
    item.status.includes('DEGRADED')
  ).length
  const same = Object.values(comparison).filter((item) =>
    item.status.includes('SAME')
  ).length
  const newMetrics = Object.values(comparison).filter((item) =>
    item.status.includes('NEW')
  ).length

  report += '\n## üìä Summary\n\n'
  report += `- **${improved}** metrics improved\n`
  report += `- **${degraded}** metrics degraded\n`
  report += `- **${same}** metrics unchanged\n`
  report += `- **${newMetrics}** new metrics\n`

  report += '\n_Performance comparison generated automatically_\n'

  return report
}

// CLI interface
const args = process.argv.slice(2)
const command = args[0]

switch (command) {
  case 'report': {
    const resultsPath =
      args[1] || 'performance/results/performance-metrics.json'
    const outputPath = args[2] || 'performance/results/performance-report.md'
    const baselinePath = args[3] || null

    try {
      const metrics = parsePerformanceResults(resultsPath)

      // Try to load baseline metrics if path provided
      let baselineMetrics = null
      if (baselinePath && fs.existsSync(baselinePath)) {
        try {
          baselineMetrics = JSON.parse(fs.readFileSync(baselinePath, 'utf8'))
          console.log('‚úÖ Loaded baseline metrics for comparison')
        } catch (e) {
          console.warn('‚ö†Ô∏è Could not load baseline metrics:', e.message)
        }
      }

      // Try to load bundle metrics if available
      let bundleMetrics = null
      const bundleMetricsPath = 'performance/results/bundle-metrics.json'
      if (fs.existsSync(bundleMetricsPath)) {
        try {
          bundleMetrics = JSON.parse(fs.readFileSync(bundleMetricsPath, 'utf8'))
          console.log('‚úÖ Loaded bundle metrics')
        } catch (e) {
          console.warn('‚ö†Ô∏è Could not load bundle metrics:', e.message)
        }
      }

      const report = generateMarkdownReport(
        metrics,
        'Performance Benchmark Results',
        baselineMetrics,
        bundleMetrics
      )

      fs.writeFileSync(outputPath, report)
      console.log(`Performance report generated: ${outputPath}`)

      // Also save metrics as JSON for comparisons
      fs.writeFileSync(
        'performance/results/performance-metrics.json',
        JSON.stringify(metrics, null, 2)
      )
      console.log(
        'Metrics saved as: performance/results/performance-metrics.json'
      )
    } catch (error) {
      console.error('Error generating report:', error.message)
      process.exit(1)
    }
    break
  }

  case 'compare': {
    const baselinePath = args[1] || 'performance/results/baseline-metrics.json'
    const currentPath =
      args[2] || 'performance/results/performance-metrics.json'
    const compareOutputPath =
      args[3] || 'performance/results/performance-comparison.md'

    try {
      // Check if baseline file exists
      if (!fs.existsSync(baselinePath)) {
        console.warn(`‚ö†Ô∏è Baseline metrics file not found: ${baselinePath}`)
        console.warn('Skipping comparison report generation.')

        // Generate a message explaining why comparison is unavailable
        const unavailableReport = `## ‚ö†Ô∏è Baseline Comparison Unavailable

Baseline performance metrics from main branch are not available. This may be because:
- The main branch has a different project structure
- Performance tests are not set up on the main branch
- The baseline measurement step encountered an error

Current branch performance metrics are still available in the report above.`

        fs.writeFileSync(compareOutputPath, unavailableReport)
        console.log(
          `Comparison unavailable message written to: ${compareOutputPath}`
        )
        process.exit(0)
      }

      const baseline = JSON.parse(fs.readFileSync(baselinePath, 'utf8'))
      const current = JSON.parse(fs.readFileSync(currentPath, 'utf8'))

      const report = generateComparisonReport(baseline || null, current)
      fs.writeFileSync(compareOutputPath, report)

      console.log(`Performance comparison generated: ${compareOutputPath}`)
    } catch (error) {
      console.error('Error generating comparison:', error.message)
      console.error(
        'This may be expected if the main branch has a different project structure.'
      )
      process.exit(1)
    }
    break
  }

  default: {
    console.log('Usage:')
    console.log(
      '  node performance/scripts/generate-performance-report.js report [results.json] [output.md] [baseline.json]'
    )
    console.log(
      '  node performance/scripts/generate-performance-report.js compare [baseline.json] [current.json] [output.md]'
    )
    console.log('')
    console.log('Examples:')
    console.log(
      '  node performance/scripts/generate-performance-report.js report'
    )
    console.log(
      '  node performance/scripts/generate-performance-report.js report performance-metrics.json report.md baseline-metrics.json'
    )
    console.log(
      '  node performance/scripts/generate-performance-report.js compare baseline.json performance-metrics.json'
    )
    break
  }
}
