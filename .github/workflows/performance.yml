name: Performance Tests

on:
  pull_request:
    branches:
      - main
    types: [opened, synchronize, reopened, ready_for_review]
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  performance:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch full history for proper baseline comparison

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'pnpm'
          cache-dependency-path: |
            pnpm-lock.yaml
            ${{ github.event_name == 'pull_request' && 'main-branch/pnpm-lock.yaml' || '' }}

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Build library
        run: pnpm run build

      - name: Check bundle size
        run: |
          echo "Checking bundle sizes..."
          node performance/scripts/check-bundle-size.js
        continue-on-error: false

      - name: Run performance tests
        run: |
          echo "Running performance tests on current branch..."
          node performance/scripts/extract-performance-metrics.js
        continue-on-error: false

      # Note: For major rewrites, main branch comparison may fail and that's expected.
      # The workflow gracefully handles incompatible project structures.
      - name: Checkout main branch for baseline
        if: github.event_name == 'pull_request'
        id: checkout-baseline
        continue-on-error: true
        uses: actions/checkout@v4
        with:
          ref: ${{ github.base_ref }}
          path: main-branch
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Measure baseline performance
        if: github.event_name == 'pull_request'
        continue-on-error: true
        id: measure-baseline
        run: |
          echo "Measuring baseline performance on main branch..."
          echo "Base ref: ${{ github.base_ref }}"
          echo "Head ref: ${{ github.head_ref }}"
          
          # 1. DEFINE A SAFE SIBLING DIRECTORY
          # We define a path that is OUTSIDE the current git repository ($GITHUB_WORKSPACE)
          # This guarantees Vitest cannot see the parent config files.
          BASELINE_DIR="$(dirname $GITHUB_WORKSPACE)/baseline-temp-env"
          echo "Preparing isolated environment at: $BASELINE_DIR"

          # Clean up any previous run (just in case)
          rm -rf "$BASELINE_DIR"
          
          # Move the checked-out main branch to the isolated directory
          # We verify main-branch exists first
          if [ -d "main-branch" ]; then
            mv main-branch "$BASELINE_DIR"
            echo "‚úÖ Moved main-branch to isolated directory"
          else
            echo "‚ùå 'main-branch' directory not found. Checkout likely failed."
            if [ "${{ github.event.pull_request.head.repo.fork }}" = "true" ]; then
              echo "   ‚ö†Ô∏è This PR is from a fork. GitHub Actions may not have permission to checkout the base branch."
            fi
            echo "baseline_available=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # 2. SWITCH CONTEXT
          # From this point on, we execute commands inside the isolated directory
          cd "$BASELINE_DIR"
          echo "Current working directory: $(pwd)"

          # Check structure
          if [ ! -f "package.json" ]; then
            echo "‚ùå package.json not found in isolated dir."
            echo "baseline_available=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Check if this looks like the same project structure
          if [ ! -f "package.json" ] || [ ! -d "performance" ] || [ ! -f "pnpm-lock.yaml" ]; then
            echo "‚ö†Ô∏è Main branch appears to have different project structure (missing package.json, performance/, or pnpm-lock.yaml)."
            echo "   This is expected for major rewrites. Skipping baseline measurement."
            echo "baseline_available=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Check if package.json has similar structure (has build script and similar dependencies)
          if ! grep -q '"build"' package.json || ! grep -q '"react"' package.json; then
            echo "‚ö†Ô∏è Main branch package.json structure differs significantly from current branch."
            echo "   Skipping baseline measurement to avoid incompatible comparisons."
            echo "baseline_available=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # 3. INSTALL & BUILD (In Isolation)
          echo "Installing dependencies..."
          if ! pnpm install --frozen-lockfile; then
            echo "‚ö†Ô∏è Main branch install failed."
            echo "baseline_available=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "Building..."
          if ! pnpm run build; then
            echo "‚ö†Ô∏è Main branch build failed."
            echo "baseline_available=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Check bundle size (optional, non-blocking)
          echo "Checking bundle size..."
          node performance/scripts/check-bundle-size.js || echo "Bundle check failed, continuing..."
          
          # Copy bundle metrics if generated
          if [ -f "performance/results/bundle-metrics.json" ]; then
            # Copy back to the ORIGINAL workspace
            mkdir -p "$GITHUB_WORKSPACE/performance/results/"
            cp performance/results/bundle-metrics.json "$GITHUB_WORKSPACE/performance/results/bundle-metrics-main.json"
            echo "‚úÖ Baseline bundle metrics saved"
          fi

          # 4. RUN PERFORMANCE TESTS (In Isolation)
          echo "Running performance tests..."
          mkdir -p performance/results/
          
          # This should now work because there are no parent config files to find!
          if ! node performance/scripts/extract-performance-metrics.js; then
            echo "‚ö†Ô∏è Failed to run performance tests on main branch."
            echo "baseline_available=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # 5. SAVE RESULTS
          # Copy the result file back to the GitHub Workspace
          if [ -f "performance/results/performance-metrics.json" ]; then
            echo "‚úÖ Baseline metrics generated. Copying to workspace..."
            mkdir -p "$GITHUB_WORKSPACE/performance/results/"
            cp performance/results/performance-metrics.json "$GITHUB_WORKSPACE/performance/results/performance-metrics-main.json"
            
            # Verify the file landed safely
            if [ -f "$GITHUB_WORKSPACE/performance/results/performance-metrics-main.json" ]; then
              echo "baseline_available=true" >> $GITHUB_OUTPUT
              echo "‚úÖ Baseline performance metrics saved successfully"
            else 
              echo "‚ùå File copy failed."
              echo "baseline_available=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "‚ö†Ô∏è Performance metrics file not generated."
            echo "baseline_available=false" >> $GITHUB_OUTPUT
          fi
          
          # 6. CLEANUP
          cd "$GITHUB_WORKSPACE"
          rm -rf "$BASELINE_DIR"
          echo "‚úÖ Cleaned up isolated environment"

      - name: Verify baseline metrics
        if: github.event_name == 'pull_request'
        continue-on-error: true
        run: |
          echo "Verifying baseline metrics availability..."
          if [ -f "performance/results/performance-metrics-main.json" ]; then
            echo "‚úÖ Baseline metrics file exists"
            echo "File size: $(wc -c < performance/results/performance-metrics-main.json) bytes"
            echo "Baseline metrics preview:"
            head -20 performance/results/performance-metrics-main.json || true
          else
            echo "‚ùå Baseline metrics file NOT found at: performance/results/performance-metrics-main.json"
            echo "Listing performance/results directory:"
            ls -la performance/results/ || echo "Directory does not exist"
            echo ""
            echo "Baseline measurement step output:"
            echo "  baseline_available=${{ steps.measure-baseline.outputs.baseline_available || 'not set' }}"
          fi

      - name: Generate performance report
        id: performance-report
        run: |
          echo "Generating performance report..."

          # Check if baseline metrics exist and include them in the report
          if [ -f "performance/results/performance-metrics-main.json" ]; then
            echo "Including baseline metrics in report..."
            node performance/scripts/generate-performance-report.js report performance/results/performance-metrics.json performance/results/performance-report.md performance/results/performance-metrics-main.json
          else
            echo "No baseline metrics found, generating report without comparison..."
            node performance/scripts/generate-performance-report.js report performance/results/performance-metrics.json performance/results/performance-report.md
          fi

          # Set output for GitHub Actions
          echo "report<<EOF" >> $GITHUB_OUTPUT
          cat performance/results/performance-report.md >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Generate comparison report
        if: github.event_name == 'pull_request'
        id: comparison-report
        continue-on-error: true
        run: |
          echo "Generating performance comparison report..."
          echo "Checking for baseline metrics file..."
          echo "Baseline available from step: ${{ steps.measure-baseline.outputs.baseline_available || 'not set' }}"

          # Check if baseline metrics exist
          if [ ! -f "performance/results/performance-metrics-main.json" ]; then
            echo "‚ö†Ô∏è Baseline metrics not found. Skipping comparison report."
            echo "Listing performance/results directory contents:"
            ls -la performance/results/ || echo "Directory does not exist"
            echo ""
            echo "Possible reasons:"
            echo "1. Baseline measurement step failed (check logs above)"
            echo "2. Main branch has different project structure"
            echo "3. Performance tests failed on main branch"
            echo ""
            echo "comparison<<EOF" >> $GITHUB_OUTPUT
            echo "## ‚ö†Ô∏è Baseline Comparison Unavailable" >> $GITHUB_OUTPUT
            echo "" >> $GITHUB_OUTPUT
            echo "Baseline performance metrics from main branch are not available. This may be because:" >> $GITHUB_OUTPUT
            echo "- The main branch has a different project structure or dependencies" >> $GITHUB_OUTPUT
            echo "- Performance tests are not compatible between branches" >> $GITHUB_OUTPUT
            echo "- Build processes differ significantly between branches" >> $GITHUB_OUTPUT
            echo "- This is a major rewrite where direct performance comparison isn't meaningful" >> $GITHUB_OUTPUT
            echo "- The baseline measurement step encountered an error (check workflow logs)" >> $GITHUB_OUTPUT
            echo "" >> $GITHUB_OUTPUT
            echo "Current branch performance metrics are still available in the report above." >> $GITHUB_OUTPUT
            echo "" >> $GITHUB_OUTPUT
            echo "**Debug Info:**" >> $GITHUB_OUTPUT
            echo "- Baseline step output: \`baseline_available=${{ steps.measure-baseline.outputs.baseline_available || 'not set' }}\`" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
            exit 0
          fi

          node performance/scripts/generate-performance-report.js compare performance/results/performance-metrics-main.json performance/results/performance-metrics.json performance/results/performance-comparison.md

          # Set output for GitHub Actions
          echo "comparison<<EOF" >> $GITHUB_OUTPUT
          cat performance/results/performance-comparison.md >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Check performance regression
        if: github.event_name == 'pull_request'
        continue-on-error: true
        run: |
          echo "Checking for significant performance regressions..."
          node -e "
          const fs = require('fs');

          // Check if baseline file exists
          if (!fs.existsSync('performance/results/performance-metrics-main.json')) {
            console.log('‚ö†Ô∏è Baseline metrics not found. Skipping regression check.');
            console.log('This is expected for major rewrites where the main branch has different structure.');
            process.exit(0);
          }

          // For major rewrites, performance comparison may not be meaningful
          // Check if the metrics look fundamentally different (e.g., very different values)
          try {
            const baseline = JSON.parse(fs.readFileSync('performance/results/performance-metrics-main.json', 'utf8'));
            const current = JSON.parse(fs.readFileSync('performance/results/performance-metrics.json', 'utf8'));

            // If most metrics have very large differences (>500% or >10x), this might be a rewrite
            let largeDifferences = 0;
            let totalMetrics = 0;

            Object.entries(current).forEach(([key, val]) => {
              const base = baseline[key];
              if (!base || key.includes('Calls')) return; // Skip missing metrics and call counts

              totalMetrics++;
              const ratio = val > base ? val / base : base / val;
              if (ratio > 5) { // More than 5x difference
                largeDifferences++;
              }
            });

            if (totalMetrics > 0 && largeDifferences / totalMetrics > 0.7) { // 70%+ of metrics have large differences
              console.log('‚ö†Ô∏è Detected potentially incompatible performance metrics between branches.');
              console.log('   Large differences in most metrics suggest this may be a major rewrite.');
              console.log('   Skipping regression check as direct performance comparison is not meaningful.');
              process.exit(0);
            }
          } catch (error) {
            console.log('‚ö†Ô∏è Could not analyze metric compatibility:', error.message);
            console.log('   Proceeding with regression check anyway...');
          }

          try {
            const baseline = JSON.parse(fs.readFileSync('performance/results/performance-metrics-main.json', 'utf8'));
            const current = JSON.parse(fs.readFileSync('performance/results/performance-metrics.json', 'utf8'));

            let hasRegression = false;
            const regressions = [];

            Object.entries(current).forEach(([key, val]) => {
              const base = baseline[key];
              if (!base || key.includes('Calls')) return; // Skip missing metrics and call counts

              const percentChange = ((val - base) / base) * 100;
              const absoluteChange = val - base;

              // Skip metrics that are too small to be meaningful (< 1ms)
              if (base < 1 && val < 1) return;

              // Flag regressions that are both significant percentage AND absolute change
              // OR large absolute changes regardless of percentage
              const significantRegression = (
                (percentChange > 15 && absoluteChange > 2) || // 15% + 2ms minimum
                absoluteChange > 5 // OR 5ms absolute change
              );

              if (significantRegression) {
                hasRegression = true;
                regressions.push(\`‚ùå \${key}: \${percentChange.toFixed(1)}% slower (\${base.toFixed(2)}ms ‚Üí \${val.toFixed(2)}ms)\`);
              }
            });

            if (hasRegression) {
              console.log('üö® PERFORMANCE REGRESSIONS DETECTED:');
              regressions.forEach(reg => console.log(\`  \${reg}\`));
              console.log('');
              console.log('These changes exceed the 15% performance regression threshold.');
              console.log('Consider optimizing before merging.');
              process.exit(1);
            } else {
              console.log('‚úÖ No significant performance regressions detected.');
            }
          } catch (error) {
            console.log('‚ö†Ô∏è Could not check for regressions:', error.message);
            console.log('Continuing without regression check...');
            process.exit(0);
          }
          "

      - name: Comment PR with performance results
        if: github.event_name == 'pull_request'
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const report = process.env.REPORT || 'Performance report not available';
            const comparison = process.env.COMPARISON || 'Baseline comparison not available - expected for major rewrites';

            const fullReport = report + '\n\n---\n\n' + comparison;

            // Check if we already commented
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const existingComment = comments.data.find(comment =>
              comment.body.includes('Performance Benchmark Results') &&
              comment.user.type === 'Bot'
            );

            if (existingComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: fullReport
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: fullReport
              });
            }
        env:
          REPORT: ${{ steps.performance-report.outputs.report }}
          COMPARISON: ${{ steps.comparison-report.outputs.comparison || 'Baseline comparison not available - expected for major rewrites' }}

      - name: Upload performance artifacts
        uses: actions/upload-artifact@v4
        with:
          name: performance-results-${{ github.sha }}
          path: |
            performance/results/performance-metrics.json
            performance/results/performance-report.md
            ${{ github.event_name == 'pull_request' && 'performance/results/performance-comparison.md' || '' }}
            ${{ github.event_name == 'pull_request' && 'performance/results/performance-metrics-main.json' || '' }}
            ${{ github.event_name == 'pull_request' && 'performance/results/bundle-metrics-main.json' || '' }}
