name: Performance Tests

on:
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened, ready_for_review]
  push:
    branches: [main]
  workflow_dispatch:

jobs:
  performance:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout code (Current Branch)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # --- SETUP ENVIRONMENT ---
      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'pnpm'
          cache-dependency-path: |
            pnpm-lock.yaml
            ${{ github.event_name == 'pull_request' && 'main-branch/pnpm-lock.yaml' || '' }}

      # --- CURRENT BRANCH ---
      - name: Install & Build (Current Branch)
        run: |
          pnpm install --frozen-lockfile
          pnpm run build

      - name: Run Checks (Current Branch)
        run: |
          echo "Checking bundle size..."
          node performance/scripts/check-bundle-size.js || true

          echo "Running performance tests..."
          node performance/scripts/extract-performance-metrics.js

      # --- BASELINE COMPARISON SECTION ---

      - name: Checkout main branch (Baseline)
        if: github.event_name == 'pull_request'
        uses: actions/checkout@v4
        with:
          ref: ${{ github.base_ref }}
          path: main-branch
          fetch-depth: 0

      # Grant explicit permissions for consistency and to avoid "permission denied" errors
      - name: Grant execution permissions to scripts
        if: github.event_name == 'pull_request'
        run: |
          chmod +x performance/scripts/measure-baseline.sh
          chmod +x performance/scripts/check-bundle-size.js
          chmod +x performance/scripts/extract-performance-metrics.js

      - name: Measure Baseline (Isolated)
        if: github.event_name == 'pull_request'
        id: measure-baseline
        run: source performance/scripts/measure-baseline.sh

      - name: Generate Reports
        id: performance-report
        run: |
          # Define a standard unavailability message for reuse
          COMPARISON_UNAVAILABLE_MESSAGE="## ‚ö†Ô∏è Baseline (main) Comparison Unavailable\n\nBaseline (main) data could not be generated from the main branch. This is expected if the main branch structure or performance scripts are incompatible with the current PR."

          # 1. Generate Comparison (If Baseline Exists)
          if [ -f "performance/results/performance-metrics-main.json" ]; then
            node performance/scripts/generate-performance-report.js compare \
              performance/results/performance-metrics-main.json \
              performance/results/performance-metrics.json \
              performance/results/performance-comparison.md
            
            # Set the comparison output from the generated file
            echo "comparison<<EOF" >> $GITHUB_OUTPUT
            cat performance/results/performance-comparison.md >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            # Set the comparison output to the unavailability message
            echo "comparison<<EOF" >> $GITHUB_OUTPUT
            echo "$COMPARISON_UNAVAILABLE_MESSAGE" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          fi

          # 2. Generate Main Report (Always runs, uses 4 args if comparison file exists)
          if [ -f "performance/results/performance-comparison.md" ]; then
            # Generate the main report *with* comparison data
            node performance/scripts/generate-performance-report.js report \
              performance/results/performance-metrics.json \
              performance/results/performance-report.md \
              performance/results/performance-metrics-main.json
          else
            # Generate the main report *without* comparison data
            node performance/scripts/generate-performance-report.js report \
              performance/results/performance-metrics.json \
              performance/results/performance-report.md
          fi

          # Set the main report output
          echo "report<<EOF" >> $GITHUB_OUTPUT
          cat performance/results/performance-report.md >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Check Regression
        if: github.event_name == 'pull_request'
        run: node performance/scripts/check-regression.js

      - name: Comment PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          # Cleaned up script: simply uses the generated outputs
          script: |
            const report = process.env.REPORT || 'Report unavailable';
            const comparison = process.env.COMPARISON || '## ‚ö†Ô∏è Baseline (main) Comparison Unavailable\n\nBaseline (main) comparison output missing.';
            const body = `## üöÄ Performance Benchmark Results\n\n${report}\n\n---\n\n${comparison}`;

            const { owner, repo } = context.repo;
            const issue_number = context.issue.number;

            const comments = await github.rest.issues.listComments({ owner, repo, issue_number });
            const botComment = comments.data.find(c => c.body.includes('Performance Benchmark Results') && c.user.type === 'Bot');

            if (botComment) {
              await github.rest.issues.updateComment({ owner, repo, comment_id: botComment.id, body });
            } else {
              await github.rest.issues.createComment({ owner, repo, issue_number, body });
            }
        env:
          REPORT: ${{ steps.performance-report.outputs.report }}
          COMPARISON: ${{ steps.performance-report.outputs.comparison }}

      - name: Upload Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: performance/results/
